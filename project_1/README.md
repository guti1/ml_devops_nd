# Predict Customer Churn

- Project **Predict Customer Churn** of ML DevOps Engineer Nanodegree Udacity

## Project Description
In the first project of the ND we try to predict the churn of credit card customers. The data we use is stemming from
[Kaggle](https://www.kaggle.com/sakshigoyal7/credit-card-customers/code). Our starting point is the
[churn notebook](churn_notebook.ipynb), the aim is to use that as base for refactoring it into a python project which
follows coding (PEP8) and engineering best practices for implementing software (modular, documented, and tested).

The project folder has the following structure:

```bash
└── project_1
    ├── README.md
    ├── churn_library_solution.py
    ├── churn_library_orig.py
    ├── churn_notebook.ipynb
    ├── churn_script_logging_and_tests.py
    ├── churn_script_logging_and_tests_orig.py
    ├── data
    │   └── bank_data.csv
    ├── images
    │   ├── eda
    │   └── results
    ├── logs
    ├── models
    │   ├── lr_model.pkl
    │   └── rfc_model.pkl
    ├── poetry.lock
    └── pyproject.toml
```

Note the files with the `_orig.py` prefixes, these are the original project files, the ones without the prefix contains 
the solutions.

During the project pipeline as first step we import the data, and do some preprocessing steps as we transform our target
variable to a numeric(binary) value. As next step we generate the exploratory plots about the distribution of our 
variables. As next step the feature engineering part is done as we encode the target variable in our features.
Next the model fitting process is executed as fitting the logistic regression model, and we also fit a random-forest
model over a pre-specified grid of the parameters via cross-validation over 5 folds. Afterwards the reports about the 
expected model-performance is generated along with some model diagnostic plots to be able to compare the performance of 
the fitted models. Lastly the feature importances are also plotted, and the fitted model object are saved as pickled
artefacts as well.


## Running Files
We use pyenv and poetry for dependency management. Thus pyenv should be available on your system (e.g. via Homebrew).
We use thy python version 3.8.12 for this project. 
- install the needed python version via virtualenv ` pyenv install 3.8.12 `
- create the virtualenv for the project as `pyenv virtualenv 3.8.12 ml_devops_p1`
- activate the virtualenv as `pyenv activate ml_devops_p1`
- make sure poetry is available e.g. `pip install poetry`
- install all the projects dependencies as `poetry install` in the project root

In case you need additional packages you can add them as `poetry add package-name==version`, optionally with exact 
versions. Packages can be updated also with poetry as `poetry update`. 

As the project dependencies are available on your interpreter you can execute the pipeline by 
`python churn_library_solution.py` or by `python churn_script_logging_and_tests.py` in oder to test out the entire 
pipeline.

## Results

The results generated by the execution of the pipeline (artefacts, plots, logs) can be found in the above shown folder 
structure, in the `images`, `logs`, `models` folders and in their sub-folders respectively. 

- `images/eda` the plots are representing the results of the initial, exploratory analysis.
- `images/results` contains the diagnostic plots for the fitted models and the feature importance as well
- `models` this folder contains the fitted models as pickled objects for the logitsic regression and the random forest
as well. 
- `logs` folder contains the logs of the pipeline execution.


